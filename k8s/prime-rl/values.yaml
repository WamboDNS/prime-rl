# Default values for prime-rl
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# Global settings
namespace: default

# Docker image configuration
image:
  repository: primeintellect/prime-rl
  pullPolicy: IfNotPresent
  tag: "main"

# Shared storage configuration
storage:
  enabled: true
  # PVC name will be automatically set to {{ .Release.Name }}-shared-data
  storageClassName: nfs
  accessModes:
    - ReadWriteMany
  size: 1Ti
  mountPath: /data

# Orchestrator component
orchestrator:
  enabled: true
  replicas: 1

  # Auto-start configuration (set to false to use sleep infinity for debugging)
  autoStart: false
  # Example command - use $INFERENCE_URL directly (comma-separated list of all inference server URLs)
  command: ""  # e.g., 'uv run orchestrator @ /app/examples/reverse_text/rl/orch.toml --output-dir /data/outputs/run_default --client.base-url $INFERENCE_URL'
  # $INFERENCE_URL is auto-set to: 'http://<release>-inference-0...:8000/v1,http://<release>-inference-1...:8000/v1,...'

  resources:
    requests:
      memory: "2Gi"
      cpu: "1"

  service:
    enabled: true
    type: ClusterIP
    port: 8000
    ncclPort: 29501

  env: []
  # - name: CUSTOM_ENV
  #   value: "value"

  nodeSelector: {}
    # nvidia.com/gpu.present: "true"  # Orchestrator doesn't need GPUs

# Inference component
inference:
  enabled: true
  replicas: 1

  # Auto-start configuration (set to false to use sleep infinity for debugging)
  autoStart: false
  command: ""  # e.g., "uv run inference @ /app/examples/reverse_text/rl/infer.toml"

  gpu:
    enabled: true
    count: 1

  resources:
    requests:
      memory: "4Gi"
      cpu: "1"

  service:
    enabled: true
    type: ClusterIP
    port: 8000

  runtimeClassName: nvidia

  # Health probes for inference server (requires /health endpoint)
  probes:
    enabled: false
    # Startup probe: allows long model loading time (up to 45 min by default)
    startup:
      periodSeconds: 10
      failureThreshold: 270  # 270 * 10s = 45 min max startup time
      timeoutSeconds: 5
    liveness:
      periodSeconds: 10
      failureThreshold: 3
      timeoutSeconds: 60
    readiness:
      periodSeconds: 5
      failureThreshold: 2
      timeoutSeconds: 5

# Trainer component
trainer:
  enabled: true
  replicas: 1

  # Auto-start configuration (set to false to use sleep infinity for debugging)
  autoStart: false
  command: ""  # e.g., "uv run trainer @ /app/examples/reverse_text/rl/train.toml --output-dir /data/outputs"

  # Helps reduce CUDA memory fragmentation with PyTorch allocator
  pytorchCudaAllocConf: "expandable_segments:True"

  gpu:
    enabled: true
    count: 1

  resources:
    requests:
      memory: "4Gi"
      cpu: "1"

  service:
    enabled: true
    type: ClusterIP
    port: 8000
    ncclPort: 29501

  env: []

  runtimeClassName: nvidia

  # Health probes for trainer (requires metrics_server config)
  probes:
    enabled: false
    # Startup probe: allows time for model loading
    startup:
      periodSeconds: 10
      failureThreshold: 60  # 60 * 10s = 10 min max startup time
      timeoutSeconds: 30
    liveness:
      periodSeconds: 30
      failureThreshold: 6   # 6 * 30s = 3 min before restart
      timeoutSeconds: 30
    readiness:
      periodSeconds: 10
      failureThreshold: 3
      timeoutSeconds: 30

# Additional configuration
config:
  # Example name (used for labeling)
  example: "reverse-text"

  # Secrets (optional)
  secrets:
    enabled: false
    name: prime-rl-secrets
    # wandbApiKey: ""
    # hfToken: ""
