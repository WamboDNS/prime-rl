# Medical SDFT with OLMo-3-7B-Think â€” Self-Distillation paper reproduction
# Reuses medical dataset, different model with longer reasoning traces
# Usage: uv run sdft @ configs/sdft/medical_olmo.toml

inference_gpu_ids = [0]
trainer_gpu_ids = [1, 2, 3]

[trainer.model]
name = "allenai/OLMo-3-7B-Think"
fused_lm_head_chunk_size = "disabled"

[trainer.data]
dataset_name = "data/medical_sdft"
prompt_field = "prompt"
teacher_prompt_field = "teacher_prompt"
batch_size = 32
micro_batch_size = 4

[trainer.loss]
alpha = 0.0  # forward KL
temperature = 1.0
num_loss_tokens_to_skip = 3

[trainer.ref_model]
enabled = true
sync_steps = 1
mixup_alpha = 0.01

[trainer.generation]
generate_from_teacher = false
max_completion_length = 4096
max_prompt_length = 1024
temperature = 1.0
num_iterations = 1

[trainer.optim]
type = "adamw"
lr = 2e-5
weight_decay = 0.0

[trainer.scheduler]
type = "cosine"
warmup_steps = 59  # 10% of max_steps (paper uses warmup_ratio=0.1)

[trainer.log]
level = "DEBUG"

[trainer.ckpt]
interval = 50

[trainer.wandb]
project = "sdft-medical-olmo"

[trainer]
max_steps = 585  # ceil(18700 / 32)
output_dir = "outputs/sdft-medical-olmo"

[inference.model]
max_model_len = 8192

[inference.server]
port = 8000
