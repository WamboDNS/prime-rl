# Medical SDFT â€” Self-Distillation paper reproduction
# Dataset: HuatuoGPT-o1 English (~18.7k train)
# Usage: uv run sdft @ configs/sdft/medical.toml

inference_gpu_ids = [0]
trainer_gpu_ids = [1, 2, 3]

[trainer.model]
name = "Qwen/Qwen2.5-7B-Instruct"
fused_lm_head_chunk_size = "disabled"

[trainer.data]
dataset_name = "data/medical_sdft"
prompt_field = "prompt"
teacher_prompt_field = "teacher_prompt"
batch_size = 32
micro_batch_size = 4

[trainer.loss]
alpha = 0.0  # forward KL
temperature = 1.0
num_loss_tokens_to_skip = 3

[trainer.ref_model]
enabled = true
sync_steps = 1
mixup_alpha = 0.01

[trainer.generation]
generate_from_teacher = false
max_completion_length = 2048
max_prompt_length = 1024
temperature = 1.0
num_iterations = 1

[trainer.optim]
type = "adamw"
lr = 2e-5
weight_decay = 0.0

[trainer.scheduler]
type = "cosine"
warmup_steps = 59  # 10% of max_steps (paper uses warmup_ratio=0.1)

[trainer.log]
level = "DEBUG"

[trainer.ckpt]
interval = 50

[trainer.wandb]
project = "sdft-medical"

[trainer]
max_steps = 585  # ceil(18700 / 32)
output_dir = "outputs/sdft-medical"

[inference.model]
max_model_len = 4096

[inference.server]
port = 8000
