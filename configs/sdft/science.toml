# Science Q&A SDFT — Self-Distillation paper reproduction
# Dataset: SciKnowEval Chemistry L3 (~150-300 train after filtering)
# Usage: uv run sdft @ configs/sdft/science.toml
# NOTE: update max_steps after running prepare_science.py

inference_gpu_ids = [0]
trainer_gpu_ids = [1, 2, 3]

[trainer.model]
name = "Qwen/Qwen2.5-7B-Instruct"
fused_lm_head_chunk_size = "disabled"

[trainer.data]
dataset_name = "data/science_sdft"
prompt_field = "prompt"
teacher_prompt_field = "teacher_prompt"
batch_size = 32
micro_batch_size = 4

[trainer.loss]
alpha = 0.0  # forward KL
temperature = 1.0
num_loss_tokens_to_skip = 3

[trainer.ref_model]
enabled = true
sync_steps = 1
mixup_alpha = 0.01

[trainer.generation]
generate_from_teacher = false
max_completion_length = 1024
max_prompt_length = 1024
temperature = 1.0
num_iterations = 1

[trainer.optim]
type = "adamw"
lr = 2e-5
weight_decay = 0.0

[trainer.scheduler]
type = "cosine"
warmup_steps = 1  # 10% of max_steps (paper uses warmup_ratio=0.1) — update with max_steps

[trainer.log]
level = "DEBUG"

[trainer.ckpt]
interval = 50

[trainer.wandb]
project = "sdft-science"

[trainer]
max_steps = 10  # placeholder — update after data prep: ceil(N / 32)
output_dir = "outputs/sdft-science"

[inference.model]
max_model_len = 4096

[inference.server]
port = 8000
